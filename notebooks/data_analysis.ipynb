{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Analysis and Preprocessing\n",
        "\n",
        "This notebook performs data loading, exploration, and preprocessing for the LitBank NER project.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/flattened_data.pkl` - Preprocessed train, validation, and test datasets\n",
        "- `data/tag_mappings.json` - NER tag mappings (tag2idx, idx2tag, PAD_TAG_IDX)\n",
        "- `data/vocabulary.json` - Word vocabulary (word2idx)\n",
        "\n",
        "**Next Step:** Use `litbank_ner_fasttext.ipynb` for model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load LitBank Dataset\n",
        "\n",
        "LitBank is loaded from Hugging Face. It contains literary texts annotated with Named entities, Coreference chains and Events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the datasets library from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load LitBank dataset from Hugging Face\n",
        "try:\n",
        "    # Load the dataset with split_0 configuration\n",
        "    dataset = load_dataset(\"coref-data/litbank_raw\", \"split_0\")\n",
        "    print(\"LitBank dataset loaded successfully!\")\n",
        "    # Display the dataset structure (train/validation/test splits)\n",
        "    print(f\"Dataset structure: {dataset}\")\n",
        "except Exception as e:\n",
        "    # If loading fails, print error message\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"You may need to download LitBank manually from https://github.com/dbamman/litbank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding the Dataset Structure\n",
        "\n",
        "LitBank has a nested structure. The following cells explore the structure and analyse the contents\n",
        "* there are 100 rows, each corresponding to a separate literary work from the period of 1719 to 1922 (for complete list see https://github.com/dbamman/litbank)\n",
        "* the dataset contains an extract of ~2000 tokens per book, the number of sentences varies according to the sentence length (see sentence length analysis below)\n",
        "* the total annotated dataset contains 210,532 tokens (rather small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# DOCUMENT-LEVEL VIEW: Show all features from first 5 documents in a table\n",
        "print(\"DOCUMENT-LEVEL OVERVIEW - ALL FEATURES\")\n",
        "\n",
        "# Convert first 5 documents to DataFrame\n",
        "df_docs = pd.DataFrame(dataset['train'][:5])\n",
        "df_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this project, I am not interested in coreference chains or events. The relevant information will be in the columns `sentences` (tokens) and `entities` (NER tags)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the first example in the training set\n",
        "if 'train' in dataset and len(dataset['train']) > 0:\n",
        "    # Get the first sample from training data\n",
        "    sample = dataset['train'][0]\n",
        "    \n",
        "    # Show the structure of sentences (nested list)\n",
        "    print(\"SENTENCES structure:\")\n",
        "    print(f\"Number of sentences in first document: {len(sample['sentences'])}\")\n",
        "    print(f\"First sentence tokens: {sample['sentences'][0]}\")\n",
        "    print(f\"Second sentence tokens: {sample['sentences'][1]}\")\n",
        "    \n",
        "    # Show the structure of entities (nested list with dicts)\n",
        "    print(\"\\nENTITIES structure:\")\n",
        "    print(f\"Number of entity lists: {len(sample['entities'])}\")\n",
        "    print(f\"Entities in first sentence: {sample['entities'][0][:5]}\")  # Show first 5 entities\n",
        "    \n",
        "else:\n",
        "    print(\"No training data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Every token has a NER tag that is either 0 (no recognised entity) or one of 6 entities: \n",
        "* Person (PER)\n",
        "* Location (LOC)\n",
        "* Facility (FAC)\n",
        "* Geopolitical entity (GPE)\n",
        "* Vehicle (VEC)\n",
        "* Organisation (ORG)  \n",
        "\n",
        "Each tag (`bio_tags`) can be of `B-type` (first token of an entity) or `I-type` (intermediate or any token following the first one in an entity that has more than one token).\n",
        "\n",
        "NB: the annotation allows for nested entities, i.e. one token can be part of several entities (e.g. in `Mary's house` the token `Mary` would be a PER as well as part of an entity LOC). For each token there is a dictionary containing the token itself and the `bio_tags`for several annotation levels. \n",
        "* Layer 0 is used for primary annotation (13.7% of tokens).\n",
        "* Layers 1-2 show nested entity annotations (fairly common, 6% of tokens).\n",
        "* Layer 3 shows deep nesting (very rare - only 0.1% of tokens).\n",
        "* Layer 4 is never used in this dataset.\n",
        "\n",
        "For simplicity in this notebook only the first layer of annotation is used for training. It is true that some of the annotation information is lost. In a more sophisticated approach it would be necessary to use all layers or combine them into a richer annotation scheme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Flatten Dataset Structure\n",
        "\n",
        "The model needs flat lists of tokens and tags. The following function will convert LitBank's nested structure into sentence-level examples.\n",
        "\n",
        "**Input**: Nested structure, organised by document. The tokens and NER tags are in the feature `entities`.  \n",
        "**Output**: Each sentence becomes its own example with a flat structure (`tokens` + `ner_tags`). Each token has exactly one tag (only layer 0 is used, see above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_litbank_data(dataset_split):\n",
        "    flattened_examples = []  # Store all flattened sentences here\n",
        "    \n",
        "    # Loop through each document in the dataset\n",
        "    for doc_idx, example in enumerate(dataset_split):\n",
        "        # Get all sentences in this document (list of token lists)\n",
        "        sentences = example.get('sentences', [])\n",
        "        # Get all entities in this document (list of entity lists per sentence)\n",
        "        entities = example.get('entities', [])\n",
        "        \n",
        "        # Process each sentence in the document\n",
        "        for sent_idx, sentence_tokens in enumerate(sentences):\n",
        "            # Skip empty sentences\n",
        "            if not sentence_tokens:\n",
        "                continue\n",
        "            \n",
        "            # Get entities for this specific sentence, entities are aligned with sentences (entities[i] corresponds to sentences[i])\n",
        "            sent_entities = entities[sent_idx] if sent_idx < len(entities) else []\n",
        "            \n",
        "            # Create a mapping from token to its NER tag, each token in sentence_tokens needs a corresponding tag\n",
        "            token_to_tag = {}  # Dictionary: token_index -> NER tag\n",
        "            \n",
        "            # Process each entity in this sentence\n",
        "            for entity_dict in sent_entities:\n",
        "                # Each entity has 'token' (the word) and 'bio_tags' (its NER label, layer 0)\n",
        "                token = entity_dict.get('token', '')\n",
        "                bio_tags = entity_dict.get('bio_tags', [])\n",
        "                \n",
        "                # bio_tags can be a list (for multi-word entities) or single string\n",
        "                if isinstance(bio_tags, list) and len(bio_tags) > 0:\n",
        "                    # Take the first tag if it's a list\n",
        "                    tag = bio_tags[0] if bio_tags[0] else 'O'\n",
        "                elif isinstance(bio_tags, str):\n",
        "                    # Use the tag directly if it's a string\n",
        "                    tag = bio_tags if bio_tags else 'O'\n",
        "                else:\n",
        "                    # Default to 'O' (outside entity) if no tag\n",
        "                    tag = 'O'\n",
        "                \n",
        "                # Find this token in the sentence and record its tag\n",
        "                # We search for the token's position to align it\n",
        "                for token_idx, sent_token in enumerate(sentence_tokens):\n",
        "                    # If token matches and hasn't been tagged yet\n",
        "                    if sent_token == token and token_idx not in token_to_tag:\n",
        "                        token_to_tag[token_idx] = tag\n",
        "                        break  # Found it, move to next entity\n",
        "            \n",
        "            # Create the final list of tags aligned with tokens, any token not in token_to_tag gets 'O' (outside entity)\n",
        "            ner_tags = [token_to_tag.get(i, 'O') for i in range(len(sentence_tokens))]\n",
        "            \n",
        "            # Add this sentence as a flat example\n",
        "            flattened_examples.append({\n",
        "                'tokens': sentence_tokens,  # List of words\n",
        "                'ner_tags': ner_tags,  # List of NER labels (same length)\n",
        "                'doc_idx': doc_idx,  # Track which document it came from\n",
        "                'sent_idx': sent_idx  # Track which sentence in the document\n",
        "            })\n",
        "    \n",
        "    return flattened_examples\n",
        "\n",
        "# Flatten all dataset splits\n",
        "train_data = flatten_litbank_data(dataset['train']) \n",
        "val_data = flatten_litbank_data(dataset['validation']) \n",
        "test_data = flatten_litbank_data(dataset['test']) \n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\nFlattened data statistics:\")\n",
        "print(f\"Training sentences: {len(train_data)}\")\n",
        "print(f\"Validation sentences: {len(val_data)}\")\n",
        "print(f\"Test sentences: {len(test_data)}\")\n",
        "\n",
        "# Show an example\n",
        "print(f\"\\nExample sentence:\")\n",
        "print(f\"Tokens: {train_data[0]['tokens'][:15]}...\")  # First 15 tokens\n",
        "print(f\"Tags:   {train_data[0]['ner_tags'][:15]}...\")  # Corresponding tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. NER Tag Distribution and Tag Mapping\n",
        "\n",
        "The next few cells analyse the types of entities that appear in LitBank and how frequently. This is important to understand the class imbalance: the training data shows a big imbalance, 86% of the tokens are tagged `0` (i.e. no entity). \n",
        "* Choosing the right evaluation metric: accuracy is probably misleading. If the model does not learn to recognise any entity, it will still be correct in 86% of cases. Precision, recall or the F1 score are better. For accuracy, 86% would be the baseline to check if the model has learned anything at all.\n",
        "* *Data strategies: Weight the loss function to make the model care more about rare classes (entities) than common ones (O) or oversample minority classes (Show the model more examples of rare entity types).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count all NER tags in the training data\n",
        "def get_tag_distribution(flattened_data):\n",
        "    \"\"\"Count how many times each NER tag appears.\"\"\"\n",
        "    all_tags = []  # Collect all tags\n",
        "    \n",
        "    # Loop through each sentence\n",
        "    for example in flattened_data:\n",
        "        # Add all tags from this sentence to our list\n",
        "        all_tags.extend(example['ner_tags'])\n",
        "    \n",
        "    # Count occurrences of each tag\n",
        "    return Counter(all_tags)\n",
        "\n",
        "# Get tag distribution from training data\n",
        "tag_dist = get_tag_distribution(train_data)\n",
        "\n",
        "# Print the distribution\n",
        "print(\"\\nNER Tag Distribution:\")\n",
        "for tag, count in sorted(tag_dist.items()):\n",
        "    # Calculate percentage\n",
        "    percentage = (count / sum(tag_dist.values())) * 100\n",
        "    print(f\"{tag:15s}: {count:6d} occurrences ({percentage:5.2f}%)\")\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Sort tags by count for better visualization\n",
        "sorted_tags = sorted(tag_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "tags, counts = zip(*sorted_tags)\n",
        "# Create bar plot\n",
        "plt.bar(range(len(tags)), counts)\n",
        "plt.xticks(range(len(tags)), tags, rotation=45, ha='right')\n",
        "plt.xlabel('NER Tag')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('NER Tag Distribution in LitBank (Training Set)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also show distribution without 'O' tag for better visibility of entities\n",
        "entity_tags = {tag: count for tag, count in tag_dist.items() if tag != 'O'}\n",
        "if entity_tags:\n",
        "    print(\"\\nEntity Tags Only (excluding 'O'):\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sorted_entity_tags = sorted(entity_tags.items(), key=lambda x: x[1], reverse=True)\n",
        "    tags, counts = zip(*sorted_entity_tags)\n",
        "    plt.bar(range(len(tags)), counts, color='steelblue')\n",
        "    plt.xticks(range(len(tags)), tags, rotation=45, ha='right')\n",
        "    plt.xlabel('Entity Tag')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Entity Tag Distribution (Excluding \"O\" Tag)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create Tag Mappings**\n",
        "\n",
        "The following cell creates two dictionaries to \"translate\" the tags to numbers and back again:\n",
        "* `tag2idx`: Tag names (like \"B-PER\") → numbers (like 0, 1, 2...)\n",
        "* `idx2tags`: Numbers → tag names (for interpreting predictions)\n",
        "\n",
        "The tag `0` will be added as a separate tag in order to avoid confusion with padding added further on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all unique tags from the training data\n",
        "all_tags = set()  # create a set to get unique values only\n",
        "for example in train_data:\n",
        "    # Add all tags from this sentence to the set\n",
        "    all_tags.update(example['ner_tags'])\n",
        "\n",
        "# Sort alphabetically, 'O' will get a valid index (not 0)\n",
        "tag_names = sorted(all_tags)\n",
        "\n",
        "# Create mapping: tag name → number (for converting labels to model input)\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(tag_names)}\n",
        "\n",
        "# Create reverse mapping: number → tag name (for converting predictions to readable tags)\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(tag_names)}\n",
        "\n",
        "# Define a special padding index that doesn't conflict with any tag (-100 is a common choice for ignored indices in PyTorch)\n",
        "PAD_TAG_IDX = -100\n",
        "\n",
        "# Print the mappings\n",
        "print(\"Tag to Index Mapping:\")\n",
        "for tag, idx in sorted(tag2idx.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {tag:15s} → {idx}\")\n",
        "\n",
        "print(f\"\\nPadding tag index: {PAD_TAG_IDX} (will be ignored in loss calculation)\")\n",
        "print(f\"Total unique tags: {len(tag_names)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Sentence Lengths and Build Vocabulary\n",
        "\n",
        "Understanding sentence length distribution helps us choose the right maximum sequence length (`MAX_LEN`) for the model (needs inputs of consistent, predictable size).  \n",
        "The hyperparameter `MAX_LEN` will determine how much shorter sentences will be padded or where longer sentences are truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate length of each sentence (number of tokens)\n",
        "sentence_lengths = [len(example['tokens']) for example in train_data]\n",
        "\n",
        "# Calculate statistics\n",
        "print(f\"Sentence Length Statistics:\")\n",
        "print(f\"  Mean:   {np.mean(sentence_lengths):.2f} tokens\")\n",
        "print(f\"  Median: {np.median(sentence_lengths):.0f} tokens\")\n",
        "print(f\"  Std:    {np.std(sentence_lengths):.2f} tokens\")\n",
        "print(f\"  Min:    {np.min(sentence_lengths)} tokens\")\n",
        "print(f\"  Max:    {np.max(sentence_lengths)} tokens\")\n",
        "\n",
        "# Calculate percentiles to help choose max_len\n",
        "percentiles = [50, 75, 90, 95, 99]\n",
        "print(f\"\\nPercentiles:\")\n",
        "for p in percentiles:\n",
        "    value = np.percentile(sentence_lengths, p)\n",
        "    print(f\"  {p}th percentile: {value:.0f} tokens\")\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(sentence_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(np.mean(sentence_lengths), color='red', linestyle='--', \n",
        "            label=f'Mean: {np.mean(sentence_lengths):.1f}')\n",
        "plt.axvline(np.median(sentence_lengths), color='green', linestyle='--', \n",
        "            label=f'Median: {np.median(sentence_lengths):.0f}')\n",
        "plt.xlabel('Sentence Length (number of tokens)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Sentence Lengths in Training Data')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Suggest a good max_len value\n",
        "recommended_max_len = int(np.percentile(sentence_lengths, 95))\n",
        "print(f\"\\nRecommended MAX_LEN: {recommended_max_len} (covers 95% of sentences)\")\n",
        "\n",
        "# Build vocabulary from training data\n",
        "def build_vocab(flattened_data, min_freq=2):\n",
        "    \"\"\"\n",
        "    Build vocabulary from training data.\n",
        "    \n",
        "    Args:\n",
        "        flattened_data: List of sentence dicts with 'tokens'\n",
        "        min_freq: Minimum frequency for a word to be included (default=2)\n",
        "                  Words appearing less than this become <UNK> (unknown)\n",
        "    \n",
        "    Returns:\n",
        "        word2idx: Dictionary mapping words to indices\n",
        "        vocab: List of all words in vocabulary\n",
        "    \"\"\"\n",
        "    # Count how many times each word appears\n",
        "    word_freq = Counter()\n",
        "    for example in flattened_data:\n",
        "        # Convert to lowercase for case-insensitive vocabulary\n",
        "        word_freq.update([token.lower() for token in example['tokens']])\n",
        "    \n",
        "    # Build vocabulary: special tokens first, then frequent words\n",
        "    # <PAD>: Padding token (index 0) - used to make all sequences same length\n",
        "    # <UNK>: Unknown token (index 1) - used for words not in vocabulary\n",
        "    vocab = ['<PAD>', '<UNK>']\n",
        "    \n",
        "    # Add words that appear at least min_freq times\n",
        "    vocab.extend([word for word, freq in word_freq.items() if freq >= min_freq])\n",
        "    \n",
        "    # Create mapping: word → index\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nTotal unique words (before filtering): {len(word_freq)}\")\n",
        "    print(f\"Vocabulary size (min_freq={min_freq}): {len(vocab)}\")\n",
        "    print(f\"Words filtered out: {len(word_freq) - (len(vocab) - 2)}\")\n",
        "    \n",
        "    return word2idx, vocab\n",
        "\n",
        "# Build vocabulary from training data\n",
        "word2idx, vocab = build_vocab(train_data, min_freq=2)\n",
        "\n",
        "# Show some example words from vocabulary\n",
        "print(f\"\\nSample vocabulary words: {vocab[2:22]}\")  # Skip <PAD> and <UNK>\n",
        "print(f\"\\nSpecial tokens:\")\n",
        "print(f\"  <PAD> index: {word2idx['<PAD>']}\")\n",
        "print(f\"  <UNK> index: {word2idx['<UNK>']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Preprocessed Data\n",
        "\n",
        "Save the preprocessed data, tag mappings, and vocabulary to the `data/` directory for use in model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "data_dir = Path('../data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save flattened datasets\n",
        "print('Saving flattened datasets...')\n",
        "with open(data_dir / 'flattened_data.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'train_data': train_data,\n",
        "        'val_data': val_data,\n",
        "        'test_data': test_data\n",
        "    }, f)\n",
        "print(f'  Saved flattened_data.pkl ({len(train_data)} train, {len(val_data)} val, {len(test_data)} test samples)')\n",
        "\n",
        "# Save tag mappings\n",
        "print('Saving tag mappings...')\n",
        "with open(data_dir / 'tag_mappings.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'tag2idx': tag2idx,\n",
        "        'idx2tag': idx2tag,\n",
        "        'PAD_TAG_IDX': PAD_TAG_IDX\n",
        "    }, f, indent=2)\n",
        "print(f'  Saved tag_mappings.json ({len(tag2idx)} tags)')\n",
        "\n",
        "# Save vocabulary\n",
        "print('Saving vocabulary...')\n",
        "with open(data_dir / 'vocabulary.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'word2idx': word2idx,\n",
        "        'vocab_size': len(word2idx)\n",
        "    }, f, indent=2)\n",
        "print(f'  Saved vocabulary.json ({len(word2idx)} words)')\n",
        "\n",
        "print('\\nAll preprocessing data saved successfully!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
